{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import scrapetube\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from colorama import Fore, Style, init\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "load_dotenv() \n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = build(\"youtube\", \"v3\", developerKey=os.getenv(\"YOUTUBE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openJson(path):\n",
    "    \n",
    "    # Creates the file if not existing\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump([], file)\n",
    "            \n",
    "    # Open it otherwise\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def saveJson(path,data):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "       json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "       #print(Style.BRIGHT+Fore.GREEN+'\\n json saved'+Style.RESET_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getvideo_details() structers the Youtube API response\n",
    "\n",
    "def getvideo_details(video_id):\n",
    "    request = youtube.videos().list(part='snippet,contentDetails', id=video_id)\n",
    "    response = request.execute()\n",
    "    videoMetadata = {\n",
    "        'id_video': response['items'][0]['id'],\n",
    "        'id_chaine':response['items'][0]['snippet']['channelId'],\n",
    "        'titre_video': response['items'][0]['snippet']['title'],\n",
    "        'description':response['items'][0]['snippet']['description'],\n",
    "        'date_publication':response['items'][0]['snippet']['publishedAt'],\n",
    "        'duree': response['items'][0]['contentDetails']['duration'],\n",
    "        'miniature':'',\n",
    "        'tags':'',\n",
    "        'langue':'',\n",
    "        'youtubeCategorie':response['items'][0]['snippet']['categoryId'],\n",
    "    }\n",
    "    \n",
    "    ################# get the highest resolution thumbnail\n",
    "    resolution_order = [\"maxres\", \"standard\", \"high\", \"medium\", \"default\"]\n",
    "    for res in resolution_order:\n",
    "        if res in response['items'][0]['snippet']['thumbnails']:\n",
    "            videoMetadata['miniature']= response['items'][0]['snippet']['thumbnails'][res]['url']\n",
    "            break\n",
    "    \n",
    "    if 'tags' in response['items'][0]['snippet']:\n",
    "        videoMetadata['tags']= response['items'][0]['snippet']['tags']\n",
    "        \n",
    "    if 'defaultAudioLanguage'in response['items'][0]['snippet']:\n",
    "        videoMetadata['langue']= response['items'][0]['snippet']['defaultAudioLanguage']\n",
    "        \n",
    "    return videoMetadata\n",
    "\n",
    "def getVideosId(listVideos):\n",
    "    ids = []\n",
    "    for videoDic in listVideos:\n",
    "        ids.append(videoDic['id_video'])\n",
    "    return ids \n",
    "   \n",
    "# updateVideos() updates the videos json file with the new videos\n",
    "\n",
    "def updateVideos(videosMetadata):\n",
    "    \n",
    "    #print(Style.BRIGHT + Fore.GREEN + '\\n Updating Videos json file...')\n",
    "    videos = openJson('./jsons/videos.json')\n",
    "    ids = getVideosId(videos) # Get the existing videos ids\n",
    "                \n",
    "    if len(videos) == 0:\n",
    "        videos.extend(videosMetadata)\n",
    "    else:\n",
    "        for video in videosMetadata:\n",
    "            id = video['id_video']\n",
    "            query = video['requete'][0]\n",
    "            if id in ids:\n",
    "                ###### Select the video dicionary by using the id\n",
    "                videoDicionary = videos[ids.index(id)]\n",
    "                videoDicionary['requete'].append(query)\n",
    "            else:\n",
    "                videos.append(video)\n",
    "                   \n",
    "    ################ Saving  \n",
    "    #print(Style.BRIGHT + Fore.YELLOW + f'\\n Actual Nbr videos : {len(videos)}')     \n",
    "    saveJson('./jsons/videos.json',videos)\n",
    "  \n",
    "# scrapeVideos() scrapes the videos given a query as input and the maximum number of videos to collect per query\n",
    "      \n",
    "def scrapeVideos(query,max_results):\n",
    "    #print(Style.BRIGHT + Fore.GREEN + '\\nScraping Videos metadata...')\n",
    "    ############################## Scrapetube returns the videos ids relevant to a search query\n",
    "    videoIds = []\n",
    "    searchResults = list(scrapetube.get_search(query,limit=max_results,sort_by='relevance',results_type='video'))\n",
    "    for result in searchResults:\n",
    "        videoIds.append(result['videoId'])\n",
    "        \n",
    "    ############################## Youtube API to featch the video metadata given the ID as input\n",
    "    videosMetadata = []\n",
    "    for videoid in videoIds:\n",
    "        videodata = getvideo_details(videoid)\n",
    "        videodata['requete']=[query]\n",
    "        videosMetadata.append(videodata)\n",
    "    \n",
    "    ############################## Update the json\n",
    "    updateVideos(videosMetadata) \n",
    "\n",
    "    #print(Style.RESET_ALL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1464"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = openJson('./jsons/queries.json')\n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping videos metadata...:   0%|          | 6/1464 [00:10<42:42,  1.76s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m tqdm(queries,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraping videos metadata...\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mscrapeVideos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 71\u001b[0m, in \u001b[0;36mscrapeVideos\u001b[1;34m(query, max_results)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscrapeVideos\u001b[39m(query,max_results):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m#print(Style.BRIGHT + Fore.GREEN + '\\nScraping Videos metadata...')\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m############################## Scrapetube returns the videos ids relevant to a search query\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     videoIds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 71\u001b[0m     searchResults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscrapetube\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43msort_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelevance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mresults_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m searchResults:\n\u001b[0;32m     73\u001b[0m         videoIds\u001b[38;5;241m.\u001b[39mappend(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideoId\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\hp\\STAYProject\\.venv\\lib\\site-packages\\scrapetube\\scrapetube.py:158\u001b[0m, in \u001b[0;36mget_search\u001b[1;34m(query, limit, sleep, sort_by, results_type)\u001b[0m\n\u001b[0;32m    154\u001b[0m api_endpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.youtube.com/youtubei/v1/search\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m videos \u001b[38;5;241m=\u001b[39m get_videos(\n\u001b[0;32m    156\u001b[0m     url, api_endpoint, results_type_map[results_type][\u001b[38;5;241m1\u001b[39m], limit, sleep\n\u001b[0;32m    157\u001b[0m )\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m videos:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m video\n",
      "File \u001b[1;32mc:\\Users\\hp\\STAYProject\\.venv\\lib\\site-packages\\scrapetube\\scrapetube.py:198\u001b[0m, in \u001b[0;36mget_videos\u001b[1;34m(url, api_endpoint, selector, limit, sleep, sort_by)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_first:\n\u001b[1;32m--> 198\u001b[0m         html \u001b[38;5;241m=\u001b[39m \u001b[43mget_initial_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m         client \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\n\u001b[0;32m    200\u001b[0m             get_json_from_html(html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINNERTUBE_CONTEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}},\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}}\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    201\u001b[0m         )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    202\u001b[0m         api_key \u001b[38;5;241m=\u001b[39m get_json_from_html(html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minnertubeApiKey\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hp\\STAYProject\\.venv\\lib\\site-packages\\scrapetube\\scrapetube.py:244\u001b[0m, in \u001b[0;36mget_initial_data\u001b[1;34m(session, url)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_initial_data\u001b[39m(session: requests\u001b[38;5;241m.\u001b[39mSession, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    243\u001b[0m     session\u001b[38;5;241m.\u001b[39mcookies\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONSENT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYES+cb\u001b[39m\u001b[38;5;124m\"\u001b[39m, domain\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.youtube.com\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 244\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mucbcb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m     html \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
      "File \u001b[1;32mc:\\Users\\hp\\STAYProject\\.venv\\lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hp\\STAYProject\\.venv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\hp\\STAYProject\\.venv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\hp\\STAYProject\\.venv\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\hp\\STAYProject\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\STAYProject\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\hp\\STAYProject\\.venv\\lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[1;32mc:\\Users\\hp\\STAYProject\\.venv\\lib\\site-packages\\urllib3\\connection.py:704\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m    706\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\STAYProject\\.venv\\lib\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\STAYProject\\.venv\\lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[0;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for query in tqdm(queries,'Scraping videos metadata...'):\n",
    "    scrapeVideos(query,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channels Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getchannel_details(channel_id):\n",
    "    request = youtube.channels().list(\n",
    "        part='snippet,contentDetails,brandingSettings',\n",
    "        id=channel_id\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = request.execute()\n",
    "        \n",
    "        if 'items' not in response:\n",
    "            print(f\"Warning: No items found for channel {channel_id}\")\n",
    "            return None\n",
    "        \n",
    "        item = response['items'][0]\n",
    "        branding = item.get('brandingSettings', {}).get('channel', {})\n",
    "        country = branding.get('country', '')\n",
    "\n",
    "        channelMetadata = {\n",
    "            'id_chaine' : item['id'],\n",
    "            'nom_chaine' : item['snippet']['title'],\n",
    "            'bio' : item['snippet']['description'],\n",
    "            'localisation' : country,\n",
    "            'date_creation' : item['snippet']['publishedAt'],\n",
    "            'type_monetisation' : '',\n",
    "            'details_monetisation' : ''\n",
    "        }\n",
    "        return channelMetadata\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for channel {channel_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# getChannnelsId() returns ids of channels from collected videos\n",
    "\n",
    "def getChannnelsId():\n",
    "    videos = openJson('./jsons/videos.json')\n",
    "    Ids = set()\n",
    "    for video in videos:\n",
    "        Ids.add(video['id_chaine'])\n",
    "    return Ids  \n",
    "    \n",
    "# scrapeChannels() scrapes channels metadatas based on the collected videos.\n",
    "\n",
    "def scrapeChannels():\n",
    "    channelIds = getChannnelsId()\n",
    "    channels = []\n",
    "    for id in tqdm(channelIds,'Scraping channels metadata...'):\n",
    "        channelDic = getchannel_details(id)\n",
    "        if channelDic is not None:\n",
    "            channels.append(channelDic)\n",
    "            \n",
    "    ################ Saving       \n",
    "    saveJson('./jsons/channels.json',channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping channels metadata...: 100%|██████████| 46/46 [00:02<00:00, 17.95it/s]\n"
     ]
    }
   ],
   "source": [
    "scrapeChannels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metrics : views and Likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getMetricsV() structeres a video metrics\n",
    "\n",
    "def getMetricsV(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet,statistics\",\n",
    "        id=video_id\n",
    "    )\n",
    "    \n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        print(f\"Error: Video {video_id} not found.\")\n",
    "        return None\n",
    "\n",
    "    item = response['items'][0]\n",
    "    \n",
    "    video_metrics = {\n",
    "        'id_video': video_id,\n",
    "        'date_releve': datetime.now().strftime('%Y-%m-%d'),\n",
    "        'nombre_vues': int(item['statistics'].get('viewCount', 0)),\n",
    "        'nombre_likes': int(item['statistics'].get('likeCount', 0)),\n",
    "    }\n",
    "\n",
    "    return video_metrics\n",
    "\n",
    "def getVIDs():\n",
    "    videos = openJson('./jsons/videos.json')\n",
    "    Ids = set()\n",
    "    for video in videos:\n",
    "        Ids.add(video['id_video'])\n",
    "    return Ids   \n",
    "\n",
    "# getVidoesMetrics() featchs all videos metrics\n",
    "\n",
    "def getVidoesMetrics():\n",
    "    videosIds = getVIDs()\n",
    "    videosMetrics = []\n",
    "    \n",
    "    for id in tqdm(videosIds,\"Scraping vidoes metrics...\"):\n",
    "        metrics = getMetricsV(id)\n",
    "        if metrics:\n",
    "            videosMetrics.append(metrics)\n",
    "              \n",
    "    saveJson('./jsons/videosMetrics.json',videosMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getVidoesMetrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channels Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metrics : Total views, subscribers, videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetricsC(channel_id):    \n",
    "    request = youtube.channels().list(\n",
    "        part='statistics',\n",
    "        id=channel_id\n",
    "    )\n",
    "    \n",
    "    response = request.execute()\n",
    "\n",
    "    if 'items' not in response or len(response['items']) == 0:\n",
    "        print(f\"Error: Channel {channel_id} not found.\")\n",
    "        return None\n",
    "\n",
    "    stats = response['items'][0]['statistics']\n",
    "    \n",
    "    metrics = {\n",
    "        'id_chaine': channel_id,\n",
    "        'date_releve': datetime.now().strftime('%Y-%m-%d'),\n",
    "        'nombre_vues': int(stats.get('viewCount', 0)),\n",
    "        'nombre_abonnes': int(stats.get('subscriberCount', 0)) if 'subscriberCount' in stats else '',\n",
    "        'nombre_videos': int(stats.get('videoCount', 0))\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def getCIDs():\n",
    "    channels = openJson('./jsons/channels.json')\n",
    "    Ids = set()\n",
    "    for channel in channels:\n",
    "        Ids.add(channel['id_chaine'])\n",
    "    return Ids   \n",
    "\n",
    "def getChannelsMetrics():\n",
    "    channelsIds = getCIDs()\n",
    "    channelsMetrics = []\n",
    "    \n",
    "    for id in tqdm(channelsIds,\"Scraping channels metrics...\"):\n",
    "        metrics = getMetricsC(id)\n",
    "        if metrics:\n",
    "            channelsMetrics.append(metrics)\n",
    "              \n",
    "    saveJson('./jsons/channelsMetrics.json',channelsMetrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getChannelsMetrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèles de Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L'idée est de chercher les mots proches sémantiquement des termes primaires ou secondaires que nous avons déjà en utilisant les embeddings de ces termes.  \n",
    "- On peut faire cela avec plusieurs embedders, qui ont également utile pour une extension sémantique par exemple :  Word2Vec, BERT, Glove..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install fasttext==0.9.3\n",
    "\n",
    "wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz\n",
    "\n",
    "gunzip cc.fr.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gunzip cc.fr.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "modelFastext = fasttext.load_model(\"/teamspace/studios/this_studio/cc.fr.300.bin\") \n",
    "\n",
    "keywords = [\"Vie en autarcie\", \"Autosuffisance\", \"Fermentation\",\"Poêle de masse\"]\n",
    "\n",
    "for keyword in keywords:\n",
    "    similar_words = modelFastext.get_nearest_neighbors(keyword, k=3)\n",
    "    print(f\"Expansions pour '{keyword}': {similar_words} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install gensim==4.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google News model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "modelGoogleNews = api.load(\"word2vec-google-news-300\") \n",
    "similar_words = modelGoogleNews.most_similar(\"fermentation\", topn=3)\n",
    "print(f\"Similar words to 'fermentation': {similar_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedder-FrWac corpus, 1.6 billion words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut0.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = 'frWac_non_lem_no_postag_no_phrase_200_cbow_cut0.bin'\n",
    "modelFrench = KeyedVectors.load_word2vec_format(model_path, binary=True, unicode_errors=\"ignore\")\n",
    "\n",
    "mots = [\"Autosuffisance\", \"Autonomie\", \"Vie en autarcie\", \"Potager\", \"Filtre Berkey\", \"Permaculture\", \n",
    "\"Filtre Doulton\", \"Poêle à bois bouilleur\", \"Poêle de masse\", \"Cuve eau pluie\", \"Maison terre-paille\", \n",
    "\"Maison torchis\", \"Ferme en pierre\", \"Maraîchage\", \"Culture lasagnes\", \"Aquaponie\", \"Agroforesterie\", \n",
    "\"Forage manuel\", \"Puits artésien\", \"Filtration naturelle\", \"Biogaz maison\", \"Kerterre\", \n",
    "\"Fermentation\", \"Fumage\"]\n",
    "\n",
    "mots_present = 0\n",
    "mots_absent = 0\n",
    "mots_similaires = []\n",
    "\n",
    "for mot in mots:\n",
    "    mot_processed = mot.replace(\" \", \"_\")\n",
    "    if mot_processed in modelFrench:\n",
    "        mots_present += 1\n",
    "        similar_words = modelFrench.most_similar(mot_processed, topn=3)\n",
    "        mots_similaires.append((mot, similar_words))\n",
    "    else:\n",
    "        mots_absent += 1\n",
    "        print(f\"Mot absent dans le modèle : {mot}\")\n",
    "\n",
    "pourcentage_absents = (mots_absent / len(mots)) * 100\n",
    "print(f\"\\nPourcentage de mots absents : {pourcentage_absents:.2f}%\")\n",
    "\n",
    "for mot, similar_words in mots_similaires:\n",
    "    print(f\"\\nMots similaires à '{mot}':\")\n",
    "    for word, similarity in similar_words:\n",
    "        print(f\"  {word}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['Mots présents', 'Mots absents']\n",
    "sizes = [mots_present, mots_absent]\n",
    "colors = ['#66b3ff', '#ff6666']\n",
    "explode = (0.1, 0)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')\n",
    "plt.title('Répartition des mots présents et absents dans le vocabulaire')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedder-FrWiki, 600 millions words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://embeddings.net/embeddings/frWiki_no_lem_no_postag_no_phrase_1000_cbow_cut100.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = 'frWiki_no_lem_no_postag_no_phrase_1000_cbow_cut100.bin'\n",
    "modelFrench = KeyedVectors.load_word2vec_format(model_path, binary=True, unicode_errors=\"ignore\")\n",
    "\n",
    "mots = [\"Autosuffisance\", \"Autonomie\", \"Vie en autarcie\", \"Potager\", \"Filtre Berkey\", \"Permaculture\", \n",
    "\"Filtre Doulton\", \"Poêle à bois bouilleur\", \"Poêle de masse\", \"Cuve eau pluie\", \"Maison terre-paille\", \n",
    "\"Maison torchis\", \"Ferme en pierre\", \"Maraîchage\", \"Culture lasagnes\", \"Aquaponie\", \"Agroforesterie\", \n",
    "\"Forage manuel\", \"Puits artésien\", \"Filtration naturelle\", \"Biogaz maison\", \"Kerterre\", \n",
    "\"Fermentation\", \"Fumage\"]\n",
    "\n",
    "mots_present = 0\n",
    "mots_absent = 0\n",
    "mots_similaires = []\n",
    "\n",
    "for mot in mots:\n",
    "    mot_processed = mot.replace(\" \", \"_\")\n",
    "    if mot_processed in modelFrench:\n",
    "        mots_present += 1\n",
    "        similar_words = modelFrench.most_similar(mot_processed, topn=3)\n",
    "        mots_similaires.append((mot, similar_words))\n",
    "    else:\n",
    "        mots_absent += 1\n",
    "        print(f\"Mot absent du modèle : {mot}\")\n",
    "\n",
    "pourcentage_absents = (mots_absent / len(mots)) * 100\n",
    "print(f\"\\nPourcentage de mots absents : {pourcentage_absents:.2f}%\")\n",
    "\n",
    "for mot, similar_words in mots_similaires:\n",
    "    print(f\"\\nMots similaires à '{mot}':\")\n",
    "    for word, similarity in similar_words:\n",
    "        print(f\"  {word}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['Mots présents', 'Mots absents']\n",
    "sizes = [mots_present, mots_absent]\n",
    "colors = ['#66b3ff', '#ff6666']\n",
    "explode = (0.1, 0)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')\n",
    "plt.title('Répartition des mots présents et absents pour Embedder-frWiki')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM and Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=os.getenv(\"GITHUB_TOKEN1\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeDetailsOne(query,max_results = 5):\n",
    "    \"Get the videos details (title, description)\"\n",
    "    #print(query)\n",
    "    videos_details = {}\n",
    "    \n",
    "    request = youtube.search().list(\n",
    "        q=query,\n",
    "        part=\"snippet\",\n",
    "        maxResults=max_results,\n",
    "        type=\"video\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "    for item in response['items']:\n",
    "        video_id = item[\"id\"][\"videoId\"]\n",
    "        title = item[\"snippet\"][\"title\"]\n",
    "        ################## Get the description with videos function\n",
    "        video_request = youtube.videos().list(\n",
    "            part=\"snippet\",\n",
    "            id=video_id\n",
    "        )\n",
    "        video_response = video_request.execute()\n",
    "        if video_response[\"items\"]:\n",
    "            description = video_response[\"items\"][0][\"snippet\"][\"description\"]\n",
    "            \n",
    "        if len(description)>0: # Keep only videos with description\n",
    "          videos_details[video_id] = {'title':title,'description':description,'query':query,'id':video_id}\n",
    "    \n",
    "    return videos_details\n",
    "\n",
    "def scrapeDetailsAll(queries):\n",
    "    \"Run the scrapeDetailsOne throw all the queries\"\n",
    "    scrape = []\n",
    "    for query in tqdm(queries, desc=\"Scraping en cours\", unit=\"requête\"):\n",
    "        videosDetails = scrapeDetailsOne(query)\n",
    "        scrape.extend(videosDetails.values())\n",
    "        \n",
    "    ################### Save in json file\n",
    "    \n",
    "    with open('scrape.json', \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(scrape, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def Prompter(descriptions,titles):\n",
    "    system_prompt = \"\"\"\n",
    "        Vous êtes un expert en autosuffisance et autonomie (alimentaire, énergétique, en eau, etc.).\n",
    "        Vous recevez en entrée des titres et descriptions de vidéos YouTube obtenus à partir de requêtes telles que :\n",
    "        ['autosuffisance tomates', 'maraîchage bio poules', 'potager en autonomie tomates', ...].\n",
    "\n",
    "        ## Types de mots-clés :\n",
    "\n",
    "        - **Mots-clés primaires** : Essentiels pour la recherche YouTube mais insuffisants seuls. Ils doivent être combinés avec des mots-clés secondaires pour obtenir des résultats pertinents.\n",
    "        - **Mots-clés secondaires** : Termes qui ne doivent pas être utilisés seuls dans les recherches YouTube sur l'autosuffisance, sous peine d'obtenir des vidéos hors sujet.\n",
    "\n",
    "        ## Exemples de mots-clés :\n",
    "\n",
    "        - **Primaires** : autosuffisance, autonomie alimentaire, autonomie énergétique, autonomie en eau, vie en autarcie, habitats autonomes...\n",
    "        - **Secondaires** : potager, filtre Berkey, permaculture, filtre Doulton, poêle à bois bouilleur, poêle de masse, cuve eau pluie, maison terre-paille, maison torchis, ferme en pierre, maraîchage, culture en lasagnes, aquaponie...\n",
    "\n",
    "        ## Votre mission :\n",
    "\n",
    "        1. **Extraire** les mots-clés primaires et secondaires présents dans les titres et descriptions fournis.\n",
    "        2. **Retourner la réponse structuré comme suit :\n",
    "        {\n",
    "            \"Primaire\": [],\n",
    "            \"Secondaire\": []\n",
    "        }\n",
    "        \n",
    "    - Tous les mots-clés extraits doivent être en minuscules.\n",
    "      \"\"\"\n",
    "    try :\n",
    "        response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Titres des vidéos : {titles}\\nDescriptions des vidéos : {descriptions}\"},\n",
    "        ],\n",
    "        temperature=0.7,  \n",
    "        top_p=1.0,\n",
    "        max_tokens=500,  \n",
    "        model=model_name\n",
    "        )\n",
    "        return  response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def Batching():\n",
    "    ################# Read the scrape.json\n",
    "    with open(\"scrape.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "      scrapeData = json.load(f)\n",
    "         \n",
    "    ################ Process in batch of 4\n",
    "    batchs = []\n",
    "    for i in range(0, len(scrapeData), 4):\n",
    "        batch = scrapeData[i:i+4]  \n",
    "\n",
    "        ##### Extract and concatenate titles and descriptions\n",
    "        titles = \"  ##############  \".join(item[\"title\"] for item in batch)\n",
    "        descriptions = \"  ######################  \".join(item[\"description\"] for item in batch)\n",
    "\n",
    "        batchs.append({\"titles\": titles, \"descriptions\": descriptions})\n",
    "    \n",
    "    ################### Save in json file\n",
    "    with open('scrapeBatches.json', \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(batchs, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def Updating(NewVoc):\n",
    "    \n",
    "    with open(\"vocabulary.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "       data = json.load(file)\n",
    "       \n",
    "    ################### Filtring\n",
    "    for keyword in NewVoc['Primaire']:\n",
    "        if keyword not in data['Primaire'] and keyword not in data['Secondaire']:\n",
    "            data['Primaire'].append(keyword)\n",
    "            \n",
    "    for keyword in NewVoc['Secondaire']:\n",
    "        if keyword not in data['Primaire'] and keyword not in data['Secondaire']:\n",
    "            data['Secondaire'].append(keyword)\n",
    "    \n",
    "    print('\\n  New Voc :  ',NewVoc)\n",
    "    print(f\"\\n len(Secondaire) :{len(data['Secondaire'])}\") \n",
    "    print(f\"\\n len(Primaire)   :  {len(data['Primaire'])}\")\n",
    "    \n",
    "    ################ Saving       \n",
    "    with open(\"vocabulary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "       json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "          \n",
    "def getExtensions():\n",
    "    ################# Read the json\n",
    "    with open(\"scrapeBatches.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "      batchs = json.load(f)\n",
    "         \n",
    "    ################ Prompter\n",
    "    nbrRequestes = 0\n",
    "    for batch in tqdm(batchs,desc=\"Augmenting...\", unit=\"batch\"):\n",
    "        llmResponse = Prompter(batch['descriptions'],batch['titles'])\n",
    "        if llmResponse:\n",
    "            json_string = llmResponse.choices[0].message.content\n",
    "            newVoc = json.loads(json_string)\n",
    "            Updating(newVoc)\n",
    "            nbrRequestes+=1\n",
    "            if nbrRequestes == 10:\n",
    "                print('----------------Sleeping for 60s ...')\n",
    "                time.sleep(60)\n",
    "                nbrRequestes=0\n",
    "        else :\n",
    "            print('---------Empty Chatgpt response')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getExtensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering Plots on Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"../jsons/videos.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    videos = json.load(file)\n",
    "    print(len(videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "echantillon = random.sample(videos, min(1000, len(videos)))\n",
    "with open(\"../jsons/echantillon.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(echantillon, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots for Vidoes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../jsons/echantillon.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    echantillon = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "categorie_labels = {\n",
    "    1: \"Film & Animation\", 2: \"Autos & Vehicles\", 10: \"Music\", 15: \"Pets & Animals\", 17: \"Sports\",\n",
    "    18: \"Short Movies\", 19: \"Travel & Events\", 20: \"Gaming\", 21: \"Videoblogging\", 22: \"People & Blogs\",\n",
    "    23: \"Comedy\", 24: \"Entertainment\", 25: \"News & Politics\", 26: \"Howto & Style\", 27: \"Education\",\n",
    "    28: \"Science & Technology\", 30: \"Movies\", 31: \"Anime/Animation\", 32: \"Action/Adventure\", 33: \"Classics\",\n",
    "    34: \"Comedy\", 35: \"Documentary\", 36: \"Drama\", 37: \"Family\", 38: \"Foreign\", 39: \"Horror\",\n",
    "    40: \"Sci-Fi/Fantasy\", 41: \"Thriller\", 42: \"Shorts\", 43: \"Shows\", 44: \"Trailers\"\n",
    "}\n",
    "\n",
    "compteur_categories = {i: 0 for i in categorie_labels.keys()}\n",
    "\n",
    "for video in echantillon:\n",
    "    cat = video.get(\"youtubeCategorie\")\n",
    "    if cat and cat.isdigit():\n",
    "        cat_id = int(cat)\n",
    "        if cat_id in compteur_categories:\n",
    "            compteur_categories[cat_id] += 1\n",
    "\n",
    "# Filtrage des catégories avec au moins 1 vidéo\n",
    "filtre = [(i, compteur_categories[i]) for i in compteur_categories if compteur_categories[i] > 0]\n",
    "\n",
    "# Calcul des pourcentages et tri décroissant\n",
    "total = sum(freq for _, freq in filtre)\n",
    "trié = sorted(filtre, key=lambda x: x[1], reverse=True)\n",
    "labels = [f\"{i} - {categorie_labels[i]}\" for i, _ in trié]\n",
    "frequences = [freq for _, freq in trié]\n",
    "\n",
    "plt.figure(figsize=(18, 8))\n",
    "bars = plt.bar(labels, frequences, color=\"#028ae4\")\n",
    "plt.xlabel(\"Catégories YouTube\")\n",
    "plt.ylabel(\"Nombre de vidéos\")\n",
    "plt.title(\"Nombre de vidéos par catégorie YouTube (échantillon trié)\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "for bar, freq in zip(bars, frequences):\n",
    "    pourcentage = (freq/1000)*100\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 2, f\"{pourcentage:.1f}%\", \n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def word_count(description):\n",
    "    return len(description.split())\n",
    "\n",
    "word_counts = [word_count(video['description']) for video in echantillon]\n",
    "\n",
    "quartiles = np.percentile(word_counts, [25, 50, 75])\n",
    "min_value = np.min(word_counts)\n",
    "max_value = np.max(word_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(word_counts, vert=True, patch_artist=True, boxprops=dict(facecolor=\"skyblue\", color=\"blue\"))\n",
    "\n",
    "plt.xlabel('Nombre de mots', fontsize=12)\n",
    "plt.title('Boxplot des longueurs de description', fontsize=14)\n",
    "\n",
    "plt.text(1.15, min_value, f\"Min: {min_value}\", fontsize=12, verticalalignment='center')\n",
    "plt.text(1.15, quartiles[0], f\"25th percentile: {quartiles[0]}\", fontsize=12, verticalalignment='center')\n",
    "plt.text(1.15, quartiles[1], f\"Median: {quartiles[1]}\", fontsize=12, verticalalignment='center')\n",
    "plt.text(1.15, quartiles[2], f\"75th percentile: {quartiles[2]}\", fontsize=12, verticalalignment='center')\n",
    "plt.text(1.15, max_value, f\"Max: {max_value}\", fontsize=12, verticalalignment='center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger les données JSON depuis le fichier\n",
    "with open(\"../jsons/echantillon.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    echantillon = json.load(f)\n",
    "\n",
    "# Fonction pour calculer le nombre de mots dans la description\n",
    "def word_count(description):\n",
    "    return len(description.split())\n",
    "\n",
    "# Obtenir le nombre de mots pour chaque description\n",
    "word_counts = [word_count(video['description']) for video in echantillon]\n",
    "\n",
    "# Définir les intervalles et initialiser un dictionnaire pour compter les vidéos dans chaque intervalle\n",
    "intervals = {\n",
    "    'moins de 10': 0,\n",
    "    '10-50': 0,\n",
    "    '50-100': 0,\n",
    "    '100-200': 0,\n",
    "    '200-500': 0,\n",
    "    'plus de 500': 0\n",
    "}\n",
    "\n",
    "# Remplir le dictionnaire avec les comptages pour chaque intervalle\n",
    "for word_count_value in word_counts:\n",
    "    if word_count_value < 10:\n",
    "        intervals['moins de 10'] += 1\n",
    "    elif 10 <= word_count_value < 50:\n",
    "        intervals['10-50'] += 1\n",
    "    elif 50 <= word_count_value < 100:\n",
    "        intervals['50-100'] += 1\n",
    "    elif 100 <= word_count_value < 200:\n",
    "        intervals['100-200'] += 1\n",
    "    elif 200 <= word_count_value < 500:\n",
    "        intervals['200-500'] += 1\n",
    "    else:\n",
    "        intervals['plus de 500'] += 1\n",
    "\n",
    "# Calculer le nombre total de vidéos\n",
    "total_videos = sum(intervals.values())\n",
    "\n",
    "# Créer l'histogramme à partir du dictionnaire\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(intervals.keys(), intervals.values(), color='skyblue', edgecolor='black')\n",
    "\n",
    "# Ajouter les labels et le titre\n",
    "plt.xlabel('Plage de nombre de mots', fontsize=12)\n",
    "plt.ylabel('Nombre de vidéos', fontsize=12)\n",
    "plt.title('Histogramme du nombre de vidéos par plage de longueur de description', fontsize=14)\n",
    "\n",
    "# Ajouter les pourcentages au-dessus de chaque barre\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    percentage = (height / total_videos) * 100\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.2, f'{percentage:.1f}%', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Afficher l'histogramme\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Charger les données JSON depuis le fichier\n",
    "with open(\"../jsons/echantillon.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    echantillon = json.load(f)\n",
    "\n",
    "# Charger les noms des chaînes depuis le fichier JSON sauvegardé\n",
    "with open(\"../jsons/channel_names.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    channel_names = json.load(f)\n",
    "\n",
    "# Comptabiliser le nombre de vidéos par chaîne\n",
    "video_count_per_channel = {}\n",
    "for video in echantillon:\n",
    "    channel_id = video['id_chaine']\n",
    "    video_count_per_channel[channel_id] = video_count_per_channel.get(channel_id, 0) + 1\n",
    "\n",
    "# Remplacer les IDs de chaînes par leurs noms\n",
    "channel_video_counts = {channel_names.get(channel_id, \"Nom non trouvé\"): count for channel_id, count in video_count_per_channel.items()}\n",
    "\n",
    "# Convertir en DataFrame pour un affichage sous forme de table\n",
    "df_channel_video_counts = pd.DataFrame(list(channel_video_counts.items()), columns=[\"Nom de la chaîne\", \"Nombre de vidéos\"])\n",
    "\n",
    "# Trier les données par nombre de vidéos (en ordre décroissant)\n",
    "df_channel_video_counts = df_channel_video_counts.sort_values(by=\"Nombre de vidéos\", ascending=False)\n",
    "\n",
    "# Sauvegarder la table dans un fichier Excel (sans l'argument 'encoding')\n",
    "df_channel_video_counts.to_excel(\"../xls/channel_video_counts.xlsx\", index=False)\n",
    "\n",
    "print(\"Le fichier Excel a été sauvegardé sous 'channel_video_counts.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Charger les données JSON depuis le fichier\n",
    "with open(\"../jsons/echantillon.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    echantillon = json.load(f)\n",
    "\n",
    "# Fonction pour convertir la durée en minutes\n",
    "def parse_duration(duration):\n",
    "    pattern = r\"PT(\\d+H)?(\\d+M)?(\\d+S)?\"\n",
    "    match = re.match(pattern, duration)\n",
    "    \n",
    "    hours = int(match.group(1)[:-1]) if match.group(1) else 0\n",
    "    minutes = int(match.group(2)[:-1]) if match.group(2) else 0\n",
    "    seconds = int(match.group(3)[:-1]) if match.group(3) else 0\n",
    "    \n",
    "    # Conversion totale en minutes\n",
    "    return (hours * 60) + minutes + (seconds / 60)\n",
    "\n",
    "# Ajouter une colonne pour la durée en minutes dans les données\n",
    "for video in echantillon:\n",
    "    video['duree_minutes'] = parse_duration(video['duree'])\n",
    "\n",
    "# Créer un DataFrame à partir des données\n",
    "df = pd.DataFrame(echantillon)\n",
    "\n",
    "# Obtenir un résumé statistique des durées des vidéos en minutes\n",
    "stats = df['duree_minutes'].describe()\n",
    "\n",
    "# Convertir les statistiques en DataFrame avec les en-têtes horizontaux\n",
    "stats_df = stats.to_frame().T  # `.T` pour transposer les statistiques en ligne\n",
    "\n",
    "# Sauvegarder les statistiques dans un fichier Excel\n",
    "output_file = \"../xls/statistiques_videos.xlsx\"\n",
    "stats_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Les statistiques ont été enregistrées dans {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger les données JSON depuis le fichier\n",
    "with open(\"../jsons/echantillon.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    echantillon = json.load(f)\n",
    "\n",
    "# Fonction pour convertir la durée en minutes\n",
    "def parse_duration(duration):\n",
    "    pattern = r\"PT(\\d+H)?(\\d+M)?(\\d+S)?\"\n",
    "    match = re.match(pattern, duration)\n",
    "    \n",
    "    hours = int(match.group(1)[:-1]) if match.group(1) else 0\n",
    "    minutes = int(match.group(2)[:-1]) if match.group(2) else 0\n",
    "    seconds = int(match.group(3)[:-1]) if match.group(3) else 0\n",
    "    \n",
    "    # Conversion totale en minutes\n",
    "    return (hours * 60) + minutes + (seconds / 60)\n",
    "\n",
    "# Ajouter une colonne pour la durée en minutes dans les données\n",
    "for video in echantillon:\n",
    "    video['duree_minutes'] = parse_duration(video['duree'])\n",
    "\n",
    "# Créer un DataFrame à partir des données\n",
    "df = pd.DataFrame(echantillon)\n",
    "\n",
    "# Définir les plages de durées\n",
    "bins = [0, 1, 5, 10, 30, 60, float('inf')]\n",
    "labels = ['< 1 min', '1-5 min', '5-10 min', '10-30 min', '30-60 min', '> 60 min']\n",
    "\n",
    "# Créer une nouvelle colonne pour les plages de durée\n",
    "df['plage_duree'] = pd.cut(df['duree_minutes'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Compter le nombre de vidéos dans chaque plage de durée\n",
    "count_by_range = df['plage_duree'].value_counts().sort_index()\n",
    "\n",
    "# Calculer le pourcentage de vidéos dans chaque plage\n",
    "total_videos = len(df)\n",
    "percent_by_range = (count_by_range / total_videos) * 100\n",
    "\n",
    "# Créer un histogramme\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = count_by_range.plot(kind='barh', color='skyblue',edgecolor='black')\n",
    "\n",
    "# Ajouter les pourcentages au-dessus de chaque barre\n",
    "for index, value in enumerate(count_by_range):\n",
    "    percentage = percent_by_range[index]\n",
    "    ax.text(value + 0.1, index, f'{percentage:.1f}%', va='center', ha='left', color='black')\n",
    "\n",
    "# Ajouter les labels et titre\n",
    "plt.xlabel('Nombre de vidéos')\n",
    "plt.ylabel('Plages de durée')\n",
    "plt.title('Répartition des vidéos par plage de durée')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Charger les données JSON depuis le fichier\n",
    "with open(\"../jsons/echantillon.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    echantillon = json.load(f)\n",
    "\n",
    "# Extraire toutes les tags dans une liste\n",
    "all_tags = []\n",
    "\n",
    "for video in echantillon:\n",
    "    tags = video.get('tags', [])\n",
    "    if isinstance(tags, list):\n",
    "        all_tags.extend(tags)\n",
    "\n",
    "# Créer un DataFrame à partir des tags\n",
    "tags_df = pd.DataFrame(all_tags, columns=['tag'])\n",
    "\n",
    "# Compter la fréquence de chaque tag\n",
    "tag_counts = tags_df['tag'].value_counts().reset_index()\n",
    "tag_counts.columns = ['tag', 'frequency']\n",
    "\n",
    "# Trier la table par fréquence en ordre décroissant\n",
    "tag_counts_sorted = tag_counts.sort_values(by='frequency', ascending=False)\n",
    "\n",
    "# Sauvegarder le résultat dans un fichier Excel\n",
    "tag_counts_sorted.to_excel('tag_frequencies.xlsx', index=False)\n",
    "\n",
    "print(\"Tableau des fréquences des tags sauvegardé dans 'tag_frequencies.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger les données JSON depuis le fichier\n",
    "with open(\"../jsons/echantillon.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    echantillon = json.load(f)\n",
    "\n",
    "# Extraire toutes les tags dans une liste\n",
    "all_tags = []\n",
    "\n",
    "for video in echantillon:\n",
    "    tags = video.get('tags', [])\n",
    "    if isinstance(tags, list):\n",
    "        all_tags.extend(tags)\n",
    "\n",
    "# Créer un DataFrame à partir des tags\n",
    "tags_df = pd.DataFrame(all_tags, columns=['tag'])\n",
    "\n",
    "# Compter la fréquence de chaque tag\n",
    "tag_counts = tags_df['tag'].value_counts()\n",
    "\n",
    "# Créer un dictionnaire des tags et de leur fréquence pour la word cloud\n",
    "tag_frequencies = tag_counts.to_dict()\n",
    "\n",
    "# Générer le nuage de mots (word cloud)\n",
    "wordcloud = WordCloud(width=1200, height=800, background_color='white').generate_from_frequencies(tag_frequencies)\n",
    "\n",
    "# Afficher le nuage de mots\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")  # Masquer les axes\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Charger les fichiers JSON\n",
    "with open(\"../jsons/videos.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    videos = json.load(f)\n",
    "\n",
    "# Compteur pour les requêtes\n",
    "query_counter = Counter()\n",
    "\n",
    "# Parcourir les vidéos et compter les occurrences des requêtes\n",
    "for video in videos:\n",
    "    if 'requete' in video:\n",
    "        for query in video['requete']:\n",
    "            query_counter[query] += 1\n",
    "\n",
    "# Trier les requêtes par fréquence décroissante\n",
    "sorted_queries = sorted(query_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Créer un DataFrame avec les résultats\n",
    "df = pd.DataFrame(sorted_queries, columns=[\"Requete\", \"Nombre d'Occurrences\"])\n",
    "\n",
    "# Sauvegarder le DataFrame dans un fichier Excel\n",
    "df.to_excel(\"../xls/requetes_triees.xlsx\", index=False)\n",
    "\n",
    "print(\"Les requêtes ont été sauvegardées dans le fichier 'requetes_triees.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "with open(\"../../jsons/videos.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    videos = json.load(f)\n",
    "\n",
    "lang_count = defaultdict(int)\n",
    "\n",
    "for video in videos:\n",
    "    lang = video.get(\"langue\", \"unknown\")\n",
    "    lang_count[lang] += 1\n",
    "\n",
    "print(f\"Nombre total de vidéos : {len(videos)}\\n\")\n",
    "print(\"Répartition par langue :\")\n",
    "for lang, count in lang_count.items():\n",
    "    print(f\"{lang} : {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "detect(\"🌱 Lacampagne est un #Ecolieu développé par David Lecoufle qui propose une #agriculture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots for Channels data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../jsons/channels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    channels = json.load(f)\n",
    "len(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../jsons/channelsMetrics.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    channelsMetrics = json.load(f)\n",
    "len(channelsMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Localisation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "locations = [channel.get('localisation', '').strip() for channel in channels if 'localisation' in channel]\n",
    "counts = {'FR': 0, 'Inconnue': 0, 'Autre': 0}\n",
    "for loc in locations:\n",
    "    if loc == 'FR':\n",
    "        counts['FR'] += 1\n",
    "    elif loc == '':\n",
    "        counts['Inconnue'] += 1\n",
    "    else:\n",
    "        counts['Autre'] += 1\n",
    "\n",
    "labels = ['France', 'Inconnue', 'Autre']\n",
    "sizes = [counts['FR'], counts['Inconnue'], counts['Autre']]\n",
    "\n",
    "colors = ['#4A90E2', '#7BADE2', '#A6C8E2']\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "wedges, texts, autotexts = plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "for text in texts:\n",
    "    text.set_fontweight('bold')\n",
    "    text.set_fontsize(12)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(12)\n",
    "\n",
    "plt.title('Répartition des localisations des chaînes YouTube', fontsize=12, fontweight='bold')\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "– « Autre » regroupe tous les autres pays en dehors de la France, comme les États-Unis, le Canada, la Chine, etc.\n",
    "- un porcentage  tres importtnat des chaines non francaise doivent etre filtré"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bio plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_lengths = [len(channel.get('bio', '').strip().split()) for channel in channels]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.boxplot(bio_lengths, patch_artist=True, boxprops=dict(facecolor='#4A90E2'))\n",
    "\n",
    "min_val = np.min(bio_lengths)\n",
    "q1 = np.percentile(bio_lengths, 25)\n",
    "median = np.median(bio_lengths)\n",
    "q3 = np.percentile(bio_lengths, 75)\n",
    "max_val = np.max(bio_lengths)\n",
    "\n",
    "plt.title(\"Distribution du nombre de mots dans les bios des chaînes\")\n",
    "plt.ylabel(\"Nombre de mots\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.text(1.2, min_val, f'Min: {min_val}', horizontalalignment='center')\n",
    "plt.text(1.2, q1, f'Q1: {q1}', horizontalalignment='center')\n",
    "plt.text(1.2, median, f'Median: {median}', horizontalalignment='center')\n",
    "plt.text(1.2, q3, f'Q3: {q3}', horizontalalignment='center')\n",
    "plt.text(1.2, max_val, f'Max: {max_val}', horizontalalignment='center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bio_lengths, columns=['bio_length'])\n",
    "\n",
    "bins = [0, 1, 10, 50, 100, float('inf')]\n",
    "labels = ['Pas de description', '0-10', '10-50', '50-100', '>100']\n",
    "df['bio_range'] = pd.cut(df['bio_length'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "count_by_range = df['bio_range'].value_counts().sort_index()\n",
    "total_bios = len(df)\n",
    "percent_by_range = (count_by_range / total_bios) * 100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = count_by_range.plot(kind='barh', color='skyblue', edgecolor='black')\n",
    "\n",
    "for index, value in enumerate(count_by_range):\n",
    "    percentage = percent_by_range[index]\n",
    "    ax.text(value + 2, index, f'{percentage:.1f}%', va='center', ha='left', color='black', fontweight='bold')\n",
    "\n",
    "plt.xlabel('Nombre de chaînes', fontweight='bold')\n",
    "plt.ylabel('Plages de longueur de bio (Nombre mots)', fontweight='bold')\n",
    "plt.title('Répartition des chaînes par longueur de bio', fontweight='bold')\n",
    "plt.xticks(rotation=0, fontweight='bold')\n",
    "plt.yticks(fontweight='bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pourcentage important dss chaines sans bio (20%) donc le filtrage sementque ne marche pas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Subscribers plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abonne_counts = [channel['nombre_abonnes'] for channel in channelsMetrics]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.boxplot(abonne_counts, patch_artist=True, boxprops=dict(facecolor='#4A90E2'))\n",
    "\n",
    "min_val = np.min(abonne_counts)\n",
    "q1 = np.percentile(abonne_counts, 25)\n",
    "median = np.median(abonne_counts)\n",
    "q3 = np.percentile(abonne_counts, 75)\n",
    "max_val = np.max(abonne_counts)\n",
    "\n",
    "plt.title(\"Distribution du nombre d'abonnés des chaînes\")\n",
    "plt.xlabel(\"Nombre d'abonnés\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.text(1, min_val, f'Min: {min_val}', horizontalalignment='center')\n",
    "plt.text(1, q1, f'Q1: {q1}', horizontalalignment='center')\n",
    "plt.text(1, median, f'Median: {median}', horizontalalignment='center')\n",
    "plt.text(1, q3, f'Q3: {q3}', horizontalalignment='center')\n",
    "plt.text(1, max_val, f'Max: {max_val}', horizontalalignment='center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "abonne_counts = [channel['nombre_abonnes'] for channel in channelsMetrics]\n",
    "\n",
    "bins = [0, 10, 100, 1000, 100000, float('inf')]\n",
    "labels = ['<10', '10-100', '100-1000', '1000-100000', '>100000']\n",
    "\n",
    "df = pd.DataFrame(abonne_counts, columns=['abonne_count'])\n",
    "df['abonne_range'] = pd.cut(df['abonne_count'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "count_by_range = df['abonne_range'].value_counts().sort_index()\n",
    "total_channels = len(df)\n",
    "percent_by_range = (count_by_range / total_channels) * 100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = count_by_range.plot(kind='barh', color='skyblue', edgecolor='black')\n",
    "\n",
    "for index, value in enumerate(count_by_range):\n",
    "    percentage = percent_by_range[index]\n",
    "    ax.text(value + 2, index, f'{percentage:.1f}%', va='center', ha='left', color='black', fontweight='bold')\n",
    "\n",
    "plt.xlabel('Nombre de chaînes', fontweight='bold')\n",
    "plt.ylabel('Plages de nombre d\\'abonnés', fontweight='bold')\n",
    "plt.title('Répartition des chaînes par nombre d\\'abonnés', fontweight='bold')\n",
    "plt.xticks(rotation=0, fontweight='bold')\n",
    "plt.yticks(fontweight='bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "178/211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24+178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "72-12+135-75+198-138+261-201+4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_counts = [channel['nombre_videos'] for channel in channelsMetrics]\n",
    "\n",
    "bins = [0, 10, 50, 100, 200, float('inf')]\n",
    "labels = ['<10', '10-50', '50-100', '100-200', '>200']\n",
    "\n",
    "df = pd.DataFrame(video_counts, columns=['video_count'])\n",
    "df['video_range'] = pd.cut(df['video_count'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "count_by_range = df['video_range'].value_counts().sort_index()\n",
    "total_channels = len(df)\n",
    "percent_by_range = (count_by_range / total_channels) * 100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = count_by_range.plot(kind='barh', color='skyblue', edgecolor='black')\n",
    "\n",
    "for index, value in enumerate(count_by_range):\n",
    "    percentage = percent_by_range[index]\n",
    "    ax.text(value + 2, index, f'{percentage:.1f}%', va='center', ha='left', color='black', fontweight='bold')\n",
    "\n",
    "plt.xlabel('Nombre de chaînes', fontweight='bold')\n",
    "plt.ylabel('Plages de nombre de vidéos', fontweight='bold')\n",
    "plt.title('Répartition des chaînes par nombre de vidéos', fontweight='bold')\n",
    "plt.xticks(rotation=0, fontweight='bold')\n",
    "plt.yticks(fontweight='bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shorts Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../jsons/shorts.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    shorts = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shorts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shorts With likn in the description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "url_pattern = re.compile(r'https?://\\S+')\n",
    "shorts_with_links = []\n",
    "\n",
    "for video in shorts:\n",
    "    description = video.get(\"description\", \"\")\n",
    "    if url_pattern.search(description):\n",
    "        shorts_with_links.append(video)\n",
    "\n",
    "with open(\"../jsons/shorts_with_links.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(shorts_with_links, outfile, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"{len(shorts_with_links)} shorts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open('../jsons/shorts_with_links.json', 'r', encoding='utf-8') as file:\n",
    "    shorts_with_links = json.load(file)\n",
    "\n",
    "def extract_links(description):\n",
    "    return re.findall(r'https?://\\S+', description)\n",
    "\n",
    "for video in shorts_with_links:\n",
    "    links = extract_links(video['description'])\n",
    "    video['links'] = links\n",
    "\n",
    "with open('../jsons/shorts_with_links.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(shorts_with_links, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Les liens ont été extraits et ajoutés avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shorts_with_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open('../jsons/shorts_with_links.json', 'r', encoding='utf-8') as file:\n",
    "    shorts_with_links = json.load(file)\n",
    "\n",
    "def contains_youtube_link(links):\n",
    "    return any(re.search(r'https://youtu\\.be/\\S+', link) for link in links)\n",
    "\n",
    "count_youtube_links = sum(1 for video in shorts_with_links if contains_youtube_link(video['links']))\n",
    "\n",
    "print(f\"Nombre de shorts contenant un lien vers une autre vidéo YouTube : {count_youtube_links}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Données\n",
    "total_shorts = 732\n",
    "shorts_with_links = 229\n",
    "shorts_with_youtube_links = 17\n",
    "shorts_with_other_links = shorts_with_links - shorts_with_youtube_links\n",
    "\n",
    "# Pourcentages pour le deuxième camembert\n",
    "youtube_link_pct = (shorts_with_youtube_links / shorts_with_links) * 100\n",
    "other_links_pct = (shorts_with_other_links / shorts_with_links) * 100\n",
    "\n",
    "# Couleurs\n",
    "colors1 = ['#66b3ff', \"#f9a2a2\"]\n",
    "colors2 = ['#99ff99', '#ffcc99']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Camembert 1 : Shorts avec/sans lien\n",
    "wedges1, texts1, autotexts1 = plt.pie(\n",
    "    [shorts_with_links, total_shorts - shorts_with_links],\n",
    "    labels=['Avec lien', 'Sans lien'],\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=colors1\n",
    ")\n",
    "# Mettre les labels et les pourcentages en gras\n",
    "for text in texts1:\n",
    "    text.set_fontweight('bold')\n",
    "    text.set_fontsize(20)\n",
    "for autotext in autotexts1:\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(20)\n",
    "\n",
    "#plt.title('Proportion de Shorts avec ou sans lien', fontsize=20, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Camembert 2 : Types de lien parmi les 229 avec lien\n",
    "wedges2, texts2, autotexts2 = plt.pie(\n",
    "    [shorts_with_youtube_links, shorts_with_other_links],\n",
    "    labels=['Lien vers vidéo YouTube', 'Autres liens'],\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=colors2\n",
    ")\n",
    "# Mettre les pourcentages en gras\n",
    "for autotext in autotexts2:\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(20)\n",
    "\n",
    "#plt.title('Répartition des types de liens (parmi les 31,3 %)', fontsize=20, fontweight='bold')\n",
    "\n",
    "# Ajuster la taille des labels du deuxième camembert\n",
    "plt.setp(plt.gca().texts, fontweight='bold', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shorts Without likn in the description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainesTv = [\n",
    "    \"France 2\", \"France 3\", \"France 4\",\"France 5\",\"Franceinfo\"\n",
    "    \"BFMTV\", \"C8\", \"CStar\", \"Gulli\", \"Cnews\",\n",
    "    \"Canal+\", \"Planète+\", \"LCI\", \"Paris première\",\n",
    "    \"6ter\", \"Arte\", \"M6\", \"W9\",\n",
    "    \"TFX\", \"TMC\", \"NRJ12\", \"TF1\",\"La Chaîne parlementaire\",\n",
    "    \"Chérie 25\", \"RMC\"\n",
    "]\n",
    "chainesTv = [nomTV.lower().replace(\" \", \"\") for nomTV in chainesTv]\n",
    "\n",
    "print(len(chainesTv))\n",
    "print(chainesTv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open('../jsons/shorts.json', 'r', encoding='utf-8') as file:\n",
    "    shorts = json.load(file)\n",
    "\n",
    "def extract_links(description):\n",
    "    return re.findall(r'https?://\\S+', description)\n",
    "\n",
    "shorts_without_links = [video for video in shorts if not extract_links(video['description'])]\n",
    "\n",
    "with open('../jsons/shorts_without_links.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(shorts_without_links, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"{len(shorts_without_links)} shorts sans lien ont été extraits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "categories = ['Tutoriel', 'Vlog rapide', 'Autre', 'Short explicatif']\n",
    "counts = [149, 93, 165, 96]\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "wedges, texts, autotexts = plt.pie(counts, labels=categories, autopct='%1.1f%%', startangle=90, colors=['#66b3ff', '#99ff99', '#ffcc99', '#ff6666'])\n",
    "\n",
    "# Mettre tout le texte en gras\n",
    "for text in texts:\n",
    "    text.set_fontweight('bold')\n",
    "    text.set_fontsize(14)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(14)\n",
    "\n",
    "plt.title('Video Categories Distribution', fontsize=16, fontweight='bold')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie chart is drawn as a circle.\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agrotube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
